{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd1bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c992c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning CUDA not Found. Using CPU\")\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 8\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc1565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import zipfile\n",
    "import os\n",
    "uploaded = files.upload()\n",
    "\n",
    "zip_path = list(uploaded.keys())[0]\n",
    "extract_dir = \"/content/data\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"extracted to\", extract_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28fa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "class PngSlices(Dataset):\n",
    "    def __init__(self, folder, size=(28,28)):\n",
    "        self.fs = sorted(glob.glob(os.path.join(folder, \"*.png\")))\n",
    "        if not self.fs:\n",
    "            raise FileNotFoundError(f\"No PNGs found in {folder}\")\n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize(size, antialias=True),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self): return len(self.fs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        img = Image.open(self.fs[i]).convert(\"L\")\n",
    "        return self.tf(img)\n",
    "\n",
    "trainset = PngSlices(\"/content/data/keras_png_slices_data/keras_png_slices_train\", size=(28,28))\n",
    "testset  = PngSlices(\"/content/data/keras_png_slices_data/keras_png_slices_test\",  size=(28,28))\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader  = DataLoader(testset,  batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"> OASIS ready: {len(trainset)} train slices, {len(testset)} test slices\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb68259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNVAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder (same as before but outputs mean and log_var)\n",
    "        self.encoder_conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 28x28 -> 28x28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 28x28 -> 14x14\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 14x14 -> 14x14\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 14x14 -> 7x7\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),  # 7x7 -> 7x7\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),  # 7x7 -> 4x4\n",
    "\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Latent space parameters\n",
    "        self.fc_mu = nn.Linear(128 * 4 * 4, latent_dim)      # Mean\n",
    "        self.fc_logvar = nn.Linear(128 * 4 * 4, latent_dim)  # Log variance\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 4 * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (128, 4, 4)),  # Reshape to (batch, 128, 4, 4)\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),  # 4x4 -> 8x8\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),  # 8x8 -> 16x16\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=1, padding=1),  # 16x16 -> 16x16\n",
    "            nn.Upsample(size=(28, 28), mode='bilinear', align_corners=False),  # 16x16 -> 28x28\n",
    "            nn.Sigmoid()  # Output in [0, 1] for image reconstruction\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode input to latent parameters\"\"\"\n",
    "        h = self.encoder_conv(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick: sample from N(mu, var) using N(0,1)\"\"\"\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        else:\n",
    "            return mu  # Use mean for inference. Using mu (mean) rather than sampling from the full distribution because:\n",
    "                        # It provides deterministic, reproducible results\n",
    "                        # The mean represents the \"most likely\" latent representation\n",
    "                        # It avoids noise that could make interpolation less smooth\n",
    "\n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent variable to reconstruction\"\"\"\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar\n",
    "\n",
    "def vae_loss_function(recon_x, x, mu, logvar, beta=1.0):\n",
    "    \"\"\"\n",
    "    VAE loss = Reconstruction loss + KL divergence\n",
    "    beta: weight for KL divergence (beta-VAE)\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (Binary Cross Entropy)\n",
    "    BCE = nn.functional.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "\n",
    "    # KL divergence loss\n",
    "    # KLD = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + beta * KLD, BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbba269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training VAE...\")\n",
    "for epoch in range(num_vae_epochs):\n",
    "    vae.train()\n",
    "    total_loss = total_bce = total_kld = 0.0\n",
    "\n",
    "    for batch in train_loader:                 # <-- no tuple unpack\n",
    "        images = batch.to(device)              # batch is a tensor\n",
    "        recon_images, mu, logvar = vae(images)\n",
    "        loss, bce, kld = vae_loss_function(recon_images, images, mu, logvar, beta)\n",
    "\n",
    "        vae_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        vae_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_bce  += bce.item()\n",
    "        total_kld  += kld.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    avg_bce  = total_bce  / len(train_loader.dataset)\n",
    "    avg_kld  = total_kld  / len(train_loader.dataset)\n",
    "    print(f'VAE Epoch [{epoch+1}/{num_vae_epochs}], Loss: {avg_loss:.4f}, BCE: {avg_bce:.4f}, KLD: {avg_kld:.4f}')\n",
    "\n",
    "    # viz\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        test_batch = next(iter(test_loader))   # <-- no tuple unpack\n",
    "        test_images = test_batch.to(device)\n",
    "        recon_images, _, _ = vae(test_images)\n",
    "\n",
    "        fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "        for i in range(10):\n",
    "            axes[0, i].imshow(test_images[i].cpu().squeeze(), cmap='gray'); axes[0, i].axis('off')\n",
    "            axes[1, i].imshow(recon_images[i].cpu().squeeze(), cmap='gray'); axes[1, i].axis('off')\n",
    "        axes[0, 0].set_ylabel('Original', fontsize=12)\n",
    "        axes[1, 0].set_ylabel('Reconstructed', fontsize=12)\n",
    "        plt.suptitle(f'VAE Reconstructions - Epoch {epoch+1}', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c0aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# Generate new samples from the VAE\n",
    "print(\"Generating new samples from VAE...\")\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    # Sample from standard normal distribution\n",
    "    z = torch.randn(64, vae.latent_dim).to(device)\n",
    "    generated_images = vae.decode(z)\n",
    "\n",
    "    # Plot generated samples\n",
    "    fig, axes = plt.subplots(8, 8, figsize=(12, 12))\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            idx = i * 8 + j\n",
    "            axes[i, j].imshow(generated_images[idx].cpu().squeeze(), cmap='gray')\n",
    "            axes[i, j].axis('off')\n",
    "\n",
    "    plt.suptitle('Generated Samples from VAE', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp3710",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
